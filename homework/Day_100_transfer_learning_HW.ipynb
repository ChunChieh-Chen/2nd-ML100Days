{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
    "\n",
    "\n",
    "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
    "\n",
    "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 32)   896         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 32)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 32)   896         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 16, 16, 32)   0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 8, 8, 64)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 8, 8, 64)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_16[0][0]           \n",
      "                                                                 max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 6, 6, 128)    73856       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 2, 2, 128)    0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 512)          0           max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 512)          2048        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 512)          2048        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          65664       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           4128        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           330         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 643,786\n",
      "Trainable params: 641,482\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "num_classes = 10\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "input_layer = Input(input_shape)\n",
    "\n",
    "conv_11 = Conv2D(32, (3,3), padding='same')(input_layer)\n",
    "conv_12 = Conv2D(32, (3,3), padding='same')(conv_11)\n",
    "maxpooling_13 = MaxPooling2D((2,2))(conv_12)\n",
    "conv_14 = Conv2D(64, (3,3), padding='same')(maxpooling_13)\n",
    "conv_15 = Conv2D(64, (3,3), padding='same')(conv_14)\n",
    "maxpooling_16 = MaxPooling2D((2,2))(conv_15)\n",
    "\n",
    "conv_21 = Conv2D(32, (3,3), padding='same')(input_layer)\n",
    "#conv_22 = Conv2D(32, (3,3), padding='same')(conv_21)\n",
    "maxpooling_23 = MaxPooling2D((2,2))(conv_21)\n",
    "conv_24 = Conv2D(64, (3,3), padding='same')(maxpooling_23)\n",
    "#conv_25 = Conv2D(64, (3,3), padding='same')(conv_24)\n",
    "maxpooling_26 = MaxPooling2D((2,2))(conv_24)\n",
    "\n",
    "add_1 = Add()([maxpooling_16, maxpooling_26])\n",
    "\n",
    "conv_31 = Conv2D(128, (3,3))(add_1)\n",
    "conv_32 = Conv2D(128, (3,3))(conv_31)\n",
    "maxpooling_33 = MaxPooling2D((2,2))(conv_32)\n",
    "\n",
    "flat = Flatten()(maxpooling_33)\n",
    "\n",
    "bn_1 = BatchNormalization()(flat)\n",
    "dense_2 = Dense(512, activation=\"relu\")(bn_1)\n",
    "drop_3 = Dropout(0.25)(dense_2)\n",
    "\n",
    "bn_4 = BatchNormalization()(drop_3)\n",
    "dense_5 = Dense(128, activation=\"relu\")(bn_4)\n",
    "drop_6 = Dropout(0.25)(dense_5)\n",
    "\n",
    "bn_7 = BatchNormalization()(drop_6)\n",
    "dense_8 = Dense(32, activation=\"relu\")(bn_7)\n",
    "drop_9 = Dropout(0.25)(dense_8)\n",
    "\n",
    "out = Dense(num_classes, activation=\"softmax\")(drop_9)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "epochs = 30 # 訓練的 epochs 數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 19:53:10.345000  1784 deprecation_wrapper.py:119] From C:\\Users\\Hui\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 19:53:10.358000  1784 deprecation_wrapper.py:119] From C:\\Users\\Hui\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0811 19:53:10.447000  1784 deprecation.py:323] From C:\\Users\\Hui\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2343/2343 [==============================] - 471s 201ms/step - loss: 1.6545 - acc: 0.4042 - val_loss: 1.2497 - val_acc: 0.5423\n",
      "Epoch 2/30\n",
      "2343/2343 [==============================] - 469s 200ms/step - loss: 1.3069 - acc: 0.5423 - val_loss: 1.0674 - val_acc: 0.6278\n",
      "Epoch 3/30\n",
      "2343/2343 [==============================] - 466s 199ms/step - loss: 1.1806 - acc: 0.5938 - val_loss: 1.0109 - val_acc: 0.6535\n",
      "Epoch 4/30\n",
      "2343/2343 [==============================] - 465s 199ms/step - loss: 1.1035 - acc: 0.6227 - val_loss: 0.9638 - val_acc: 0.6846\n",
      "Epoch 5/30\n",
      "2343/2343 [==============================] - 469s 200ms/step - loss: 1.0556 - acc: 0.6429 - val_loss: 0.9501 - val_acc: 0.6823\n",
      "Epoch 6/30\n",
      "2343/2343 [==============================] - 465s 199ms/step - loss: 1.0155 - acc: 0.6565 - val_loss: 0.8288 - val_acc: 0.7148\n",
      "Epoch 7/30\n",
      "2343/2343 [==============================] - 472s 202ms/step - loss: 0.9782 - acc: 0.6704 - val_loss: 0.8646 - val_acc: 0.7056\n",
      "Epoch 8/30\n",
      "2343/2343 [==============================] - 472s 201ms/step - loss: 0.9508 - acc: 0.6825 - val_loss: 0.7928 - val_acc: 0.7296\n",
      "Epoch 9/30\n",
      "2343/2343 [==============================] - 477s 204ms/step - loss: 0.9242 - acc: 0.6904 - val_loss: 0.7707 - val_acc: 0.7369\n",
      "Epoch 10/30\n",
      "2343/2343 [==============================] - 490s 209ms/step - loss: 0.9078 - acc: 0.6976 - val_loss: 0.8194 - val_acc: 0.7248\n",
      "Epoch 11/30\n",
      "2343/2343 [==============================] - 494s 211ms/step - loss: 0.8878 - acc: 0.7033 - val_loss: 0.7645 - val_acc: 0.7387\n",
      "Epoch 12/30\n",
      "2343/2343 [==============================] - 489s 209ms/step - loss: 0.8696 - acc: 0.7107 - val_loss: 0.7749 - val_acc: 0.7390\n",
      "Epoch 13/30\n",
      "2343/2343 [==============================] - 479s 205ms/step - loss: 0.8555 - acc: 0.7172 - val_loss: 0.8896 - val_acc: 0.7173\n",
      "Epoch 14/30\n",
      "2343/2343 [==============================] - 478s 204ms/step - loss: 0.8392 - acc: 0.7216 - val_loss: 0.8027 - val_acc: 0.7409\n",
      "Epoch 15/30\n",
      "2343/2343 [==============================] - 475s 203ms/step - loss: 0.8284 - acc: 0.7256 - val_loss: 0.7146 - val_acc: 0.7619\n",
      "Epoch 16/30\n",
      "2343/2343 [==============================] - 476s 203ms/step - loss: 0.8149 - acc: 0.7315 - val_loss: 0.6884 - val_acc: 0.7634\n",
      "Epoch 17/30\n",
      "2343/2343 [==============================] - 501s 214ms/step - loss: 0.8070 - acc: 0.7337 - val_loss: 0.6537 - val_acc: 0.7807\n",
      "Epoch 18/30\n",
      "2343/2343 [==============================] - 490s 209ms/step - loss: 0.7956 - acc: 0.7375 - val_loss: 0.6606 - val_acc: 0.7797\n",
      "Epoch 19/30\n",
      "2343/2343 [==============================] - 470s 200ms/step - loss: 0.7866 - acc: 0.7405 - val_loss: 0.6520 - val_acc: 0.7825\n",
      "Epoch 20/30\n",
      "2343/2343 [==============================] - 469s 200ms/step - loss: 0.7799 - acc: 0.7438 - val_loss: 0.6447 - val_acc: 0.7835\n",
      "Epoch 21/30\n",
      "2343/2343 [==============================] - 469s 200ms/step - loss: 0.7715 - acc: 0.7461 - val_loss: 0.6103 - val_acc: 0.7947\n",
      "Epoch 22/30\n",
      "2343/2343 [==============================] - 469s 200ms/step - loss: 0.7700 - acc: 0.7480 - val_loss: 0.6480 - val_acc: 0.7867\n",
      "Epoch 23/30\n",
      "2343/2343 [==============================] - 475s 203ms/step - loss: 0.7612 - acc: 0.7509 - val_loss: 0.7037 - val_acc: 0.7667\n",
      "Epoch 24/30\n",
      "2343/2343 [==============================] - 503s 215ms/step - loss: 0.7582 - acc: 0.7526 - val_loss: 0.6417 - val_acc: 0.7824\n",
      "Epoch 25/30\n",
      "2343/2343 [==============================] - 491s 209ms/step - loss: 0.7517 - acc: 0.7551 - val_loss: 0.6270 - val_acc: 0.7909\n",
      "Epoch 26/30\n",
      "2343/2343 [==============================] - 482s 206ms/step - loss: 0.7480 - acc: 0.7546 - val_loss: 0.5871 - val_acc: 0.8007\n",
      "Epoch 27/30\n",
      "2343/2343 [==============================] - 476s 203ms/step - loss: 0.7402 - acc: 0.7590 - val_loss: 0.6440 - val_acc: 0.7841\n",
      "Epoch 28/30\n",
      "2343/2343 [==============================] - 477s 204ms/step - loss: 0.7369 - acc: 0.7597 - val_loss: 0.6444 - val_acc: 0.7864\n",
      "Epoch 29/30\n",
      "2343/2343 [==============================] - 484s 206ms/step - loss: 0.7330 - acc: 0.7597 - val_loss: 0.6279 - val_acc: 0.7856\n",
      "Epoch 30/30\n",
      "2343/2343 [==============================] - 473s 202ms/step - loss: 0.7239 - acc: 0.7645 - val_loss: 0.6288 - val_acc: 0.7876\n",
      "Test loss: 0.6288475914001465\n",
      "Test accuracy: 0.7876\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=RMSprop(),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "#model = build_model()\n",
    "model.fit_generator(data_generator.flow(x_train, y_train, batch_size=batch_size), \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=(x_train.shape[0]*3 // batch_size),\n",
    "                    verbose=1, \n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
